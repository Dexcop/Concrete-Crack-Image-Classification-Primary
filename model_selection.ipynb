{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cde9b130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from tqdm import tqdm \n",
    "from skimage import feature \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74957643",
   "metadata": {},
   "source": [
    "In the initial phases, 'SDGClassifier' is used for its efficiency on handling large datasets via incremental learning (partial_fit). However, given the change to a primary dataset's concentrated size (~2,650 images), memory constraints are no longer a concern\n",
    "\n",
    "Therefore, a transition to the normal batch training method using standard solvers (e.g. logistic regression, svc, and exact decision tree construction). Because unlike SDG, which approximates the optimal solution, these solvers calculate the global optimum (or stable local optima) by analyzing the entire dataset simultaneously. This pivot will ensure maximum stability, reproducibility, and precision, eliminating all variance associating with stochastic gradient estimation or smaller datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a84cbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'dataset_augmented'\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "027f7ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting file paths and labels\n",
      "Training test size: 1987\n",
      "Testing test size: 663\n"
     ]
    }
   ],
   "source": [
    "print('Getting file paths and labels')\n",
    "\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "positive_path = os.path.join(DATASET_PATH, 'Positive')\n",
    "negative_path = os.path.join(DATASET_PATH, 'Negative')\n",
    "\n",
    "for filename in os.listdir(positive_path):\n",
    "    image_paths.append(os.path.join(positive_path, filename))\n",
    "    labels.append(1)\n",
    "    \n",
    "for filename in os.listdir(negative_path):\n",
    "    image_paths.append(os.path.join(negative_path, filename))\n",
    "    labels.append(0)\n",
    "    \n",
    "image_paths = np.array(image_paths)\n",
    "labels = np.array(labels)\n",
    "\n",
    "X_train_paths, X_test_paths, y_train, y_test = train_test_split(\n",
    "    image_paths, labels, test_size=0.25, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f'Training test size: {len(X_train_paths)}')\n",
    "print(f'Testing test size: {len(X_test_paths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78c4290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_extract_hog(path, label):\n",
    "    data = []\n",
    "    labels = []\n",
    "    files = [f for f in os.listdir(path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    for f in tqdm(files, desc=f\"Loading label {label}\"):\n",
    "        img_path = os.path.join(path, f)\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None: continue\n",
    "        \n",
    "        image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        hog_feat = feature.hog(\n",
    "            gray, \n",
    "            orientations=9, \n",
    "            pixels_per_cell=(16, 16), \n",
    "            cells_per_block=(2, 2), \n",
    "            transform_sqrt=True, \n",
    "            block_norm='L1'\n",
    "        )\n",
    "        \n",
    "        data.append(hog_feat)\n",
    "        labels.append(label)\n",
    "        \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ff27d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. PREPARING DATA (Extracting Features) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading label 1: 100%|██████████| 1325/1325 [01:02<00:00, 21.36it/s] \n",
      "Loading label 0: 100%|██████████| 1325/1325 [01:12<00:00, 18.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Matrix Shape: (2650, 6084)\n",
      "Labels Shape: (2650,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 1. PREPARING DATA (Extracting Features) ---\")\n",
    "\n",
    "pos_data, pos_labels = load_and_extract_hog(os.path.join(DATASET_PATH, 'Positive'), 1)\n",
    "neg_data, neg_labels = load_and_extract_hog(os.path.join(DATASET_PATH, 'Negative'), 0)\n",
    "\n",
    "X = np.array(pos_data + neg_data)\n",
    "y = np.array(pos_labels + neg_labels)\n",
    "\n",
    "print(f\"\\nFeature Matrix Shape: {X.shape}\")\n",
    "print(f\"Labels Shape: {y.shape}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3c12dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. STARTING MODEL SELECTION ---\n",
      "\n",
      "Training SGD (Previous Baseline)\n",
      "\n",
      "Evaluating SGD (Previous Baseline)\n",
      "\n",
      "Training Logistic Regression\n",
      "\n",
      "Evaluating Logistic Regression\n",
      "\n",
      "Training KNN (k=5)\n",
      "\n",
      "Evaluating KNN (k=5)\n",
      "\n",
      "Training Naive Bayes\n",
      "\n",
      "Evaluating Naive Bayes\n",
      "\n",
      "Training Random Forest\n",
      "\n",
      "Evaluating Random Forest\n",
      "\n",
      "Training SVM (Linear)\n",
      "\n",
      "Evaluating SVM (Linear)\n",
      "\n",
      "Training SVM (RBF Kernel)\n",
      "\n",
      "Evaluating SVM (RBF Kernel)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 2. STARTING MODEL SELECTION ---\")\n",
    "\n",
    "models = {\n",
    "    \"SGD (Previous Baseline)\": SGDClassifier(max_iter=1000, tol=1e-3, random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"KNN (k=5)\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM (Linear)\": SVC(kernel='linear', C=1.0, probability=True, random_state=42),\n",
    "    \"SVM (RBF Kernel)\": SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f'\\nTraining {name}')\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    print(f'\\nEvaluating {name}')\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    report = classification_report(y_test, y_pred, target_names=['No Crack (0)', 'Crack (1)'], output_dict=True)\n",
    "    results[name] = report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85b80836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- FINAL MODEL SELECTION RESULTS (HOG 16x16) ---\n",
      "==========================================\n",
      "               SGD (Previous Baseline) \n",
      "==========================================\n",
      "   Accuracy                           0.68\n",
      "   Crack F1-Score                     0.62\n",
      "   No Crack F1-Score                  0.72\n",
      "   Macro Avg F1                       0.67\n",
      "\n",
      "\n",
      "==========================================\n",
      "               Logistic Regression \n",
      "==========================================\n",
      "   Accuracy                           0.73\n",
      "   Crack F1-Score                     0.72\n",
      "   No Crack F1-Score                  0.74\n",
      "   Macro Avg F1                       0.73\n",
      "\n",
      "\n",
      "==========================================\n",
      "               KNN (k=5) \n",
      "==========================================\n",
      "   Accuracy                           0.54\n",
      "   Crack F1-Score                     0.21\n",
      "   No Crack F1-Score                  0.68\n",
      "   Macro Avg F1                       0.44\n",
      "\n",
      "\n",
      "==========================================\n",
      "               Naive Bayes \n",
      "==========================================\n",
      "   Accuracy                           0.63\n",
      "   Crack F1-Score                     0.53\n",
      "   No Crack F1-Score                  0.70\n",
      "   Macro Avg F1                       0.61\n",
      "\n",
      "\n",
      "==========================================\n",
      "               Random Forest WINNER\n",
      "==========================================\n",
      "   Accuracy                           0.77\n",
      "   Crack F1-Score                     0.78\n",
      "   No Crack F1-Score                  0.76\n",
      "   Macro Avg F1                       0.77\n",
      "\n",
      "\n",
      "==========================================\n",
      "               SVM (Linear) \n",
      "==========================================\n",
      "   Accuracy                           0.72\n",
      "   Crack F1-Score                     0.70\n",
      "   No Crack F1-Score                  0.74\n",
      "   Macro Avg F1                       0.72\n",
      "\n",
      "\n",
      "==========================================\n",
      "               SVM (RBF Kernel) \n",
      "==========================================\n",
      "   Accuracy                           0.75\n",
      "   Crack F1-Score                     0.75\n",
      "   No Crack F1-Score                  0.75\n",
      "   Macro Avg F1                       0.75\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n--- FINAL MODEL SELECTION RESULTS (HOG 16x16) ---')\n",
    "\n",
    "best_model = max(results, key=lambda p: results[p]['macro avg']['f1-score'])\n",
    "\n",
    "for name, report in results.items():\n",
    "    print(\"==========================================\")\n",
    "    print(f\"               {name} {'WINNER' if name == best_model else ''}\")\n",
    "    print(\"==========================================\")\n",
    "    print(f\"   Accuracy                           {report['accuracy']:.2f}\")\n",
    "    print(f\"   Crack F1-Score                     {report['Crack (1)']['f1-score']:.2f}\")\n",
    "    print(f\"   No Crack F1-Score                  {report['No Crack (0)']['f1-score']:.2f}\")\n",
    "    print(f\"   Macro Avg F1                       {report['macro avg']['f1-score']:.2f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19971335",
   "metadata": {},
   "source": [
    "```\n",
    "==========================================\n",
    "               Random Forest WINNER\n",
    "==========================================\n",
    "   Accuracy                           0.77\n",
    "   Crack F1-Score                     0.78\n",
    "   No Crack F1-Score                  0.76\n",
    "   Macro Avg F1                       0.77\n",
    "   ```\n",
    "best result metric wise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

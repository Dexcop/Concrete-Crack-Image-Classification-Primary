{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7c14534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.feature import local_binary_pattern, hog\n",
    "from tqdm import tqdm \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e8d63f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = './Dataset_Augmented/'\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3344170b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting file paths and labels\n",
      "Training test size: 1987\n",
      "Testing test size: 663\n"
     ]
    }
   ],
   "source": [
    "print('Getting file paths and labels')\n",
    "\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "positive_path = os.path.join(DATASET_PATH, 'Positive')\n",
    "negative_path = os.path.join(DATASET_PATH, 'Negative')\n",
    "\n",
    "for filename in os.listdir(positive_path):\n",
    "    image_paths.append(os.path.join(positive_path, filename))\n",
    "    labels.append(1)\n",
    "    \n",
    "for filename in os.listdir(negative_path):\n",
    "    image_paths.append(os.path.join(negative_path, filename))\n",
    "    labels.append(0)\n",
    "    \n",
    "image_paths = np.array(image_paths)\n",
    "labels = np.array(labels)\n",
    "\n",
    "X_train_paths, X_test_paths, y_train, y_test = train_test_split(\n",
    "    image_paths, labels, test_size=0.25, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f'Training test size: {len(X_train_paths)}')\n",
    "print(f'Testing test size: {len(X_test_paths)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a608f3c7",
   "metadata": {},
   "source": [
    "# Getting the best descriptor and detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d7322a",
   "metadata": {},
   "source": [
    "## just detector: using lbp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51cdb49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generator_lbp(image_paths, labels, batch_size):\n",
    "    num_samples = len(image_paths)\n",
    "    \n",
    "    while True:\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        shuffled_paths = image_paths[indices]\n",
    "        shuffled_labels = labels[indices]\n",
    "        \n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_paths = shuffled_paths[i:i+batch_size]\n",
    "            batch_labels = shuffled_labels[i:i+batch_size]\n",
    "            \n",
    "            batch_features = []\n",
    "            \n",
    "            for img_path in tqdm(batch_paths, desc='Batch Progress'):\n",
    "                image = cv2.imread(img_path)\n",
    "                image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "                \n",
    "                gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                # gray_image_eq = cv2.equalizeHist(gray_image)\n",
    "                \n",
    "                lbp = local_binary_pattern(gray_image, P=8, R=1, method='uniform')\n",
    "                \n",
    "                (hist, _) = np.histogram(lbp.ravel(), bins = np.arange(0, 11), range=(0, 10))\n",
    "                \n",
    "                hist = hist.astype('float')\n",
    "                \n",
    "                hist /= (hist.sum() + 1e-6)\n",
    "                \n",
    "                batch_features.append(hist)\n",
    "            \n",
    "            yield np.array(batch_features), np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7a8e27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching one batch of feature vectors to test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 17.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline complete, ready for training\n",
      "shape of one batch of features: (32, 10)\n",
      "shape of one batch of labels: (32,)\n",
      "example feature vector (first image in batch:\n",
      " [0.16161113 0.11670918 0.04316805 0.03103077 0.02742347 0.03350207\n",
      " 0.04500159 0.11172672 0.17179528 0.25803173])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Verification\n",
    "train_gen_lbp = feature_generator_lbp(X_train_paths, y_train, BATCH_SIZE)\n",
    "\n",
    "print('fetching one batch of feature vectors to test')\n",
    "sample_batch_features, sample_batch_labels = next(train_gen_lbp) \n",
    "\n",
    "print('pipeline complete, ready for training')\n",
    "print(f'shape of one batch of features: {sample_batch_features.shape}') # 32 per batch and 10 length\n",
    "print(f'shape of one batch of labels: {sample_batch_labels.shape}')  # 32 per batch\n",
    "print(f'example feature vector (first image in batch:\\n {sample_batch_features[0]})') # 10 arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98d77a3",
   "metadata": {},
   "source": [
    "## detector with descriptor (fast + brief) - set up a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3905e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generator_fast(image_paths, labels, batch_size):\n",
    "    print(\"--- RUNNING THE NEW, CORRECTED FAST GENERATOR V2 ---\")\n",
    "    fast = cv2.FastFeatureDetector_create(nonmaxSuppression=False)\n",
    "    fast.setThreshold(5)\n",
    "    brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()\n",
    "    \n",
    "    num_samples = len(image_paths)\n",
    "    \n",
    "    while True:\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        shuffled_paths = image_paths[indices]\n",
    "        shuffled_labels = labels[indices]\n",
    "        \n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_paths = shuffled_paths[i:i + batch_size]\n",
    "            batch_labels = shuffled_labels[i:i + batch_size]\n",
    "            \n",
    "            batch_features = []\n",
    "            \n",
    "            print(f'processing batch at index: {i}')\n",
    "            for img_path in tqdm(batch_paths, desc='Batch Progress'):\n",
    "                image = cv2.imread(img_path)\n",
    "                image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "                \n",
    "                gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                keypoints = fast.detect(gray_image, None)\n",
    "                \n",
    "                keypoints, descriptors = brief.compute(gray_image, keypoints)\n",
    "                \n",
    "                if descriptors is not None:\n",
    "                    feature_vector = np.mean(descriptors, axis=0)\n",
    "                else:\n",
    "                    feature_vector = np.zeros(32)\n",
    "                \n",
    "                batch_features.append(feature_vector)\n",
    "            \n",
    "            yield np.array(batch_features), np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "283efb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching one batch of feature vectors to test\n",
      "--- RUNNING THE NEW, CORRECTED FAST GENERATOR V2 ---\n",
      "processing batch at index: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 19.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline complete, ready for training\n",
      "shape of one batch of features: (32, 32)\n",
      "shape of one batch of labels: (32,)\n",
      "example feature vector (first image in batch:\n",
      " [ 78.55988024  96.00698603  70.41616766 162.17065868  88.98602794\n",
      "  82.38223553 154.23752495 112.42215569  67.43013972 157.62774451\n",
      "  95.01696607  95.93313373 165.50998004 172.01696607 107.03992016\n",
      "  63.22355289 155.17764471 182.25948104 102.21457086 131.93612774\n",
      " 130.01197605 100.05688623 162.99401198  82.20958084 162.39221557\n",
      " 137.3752495  139.61377246 122.23852295 158.13972056  89.46307385\n",
      " 158.45309381 164.91616766])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Verification\n",
    "train_gen_fast = feature_generator_fast(X_train_paths, y_train, BATCH_SIZE)\n",
    "\n",
    "print('fetching one batch of feature vectors to test')\n",
    "sample_fast_batch_features, sample_fast_batch_labels = next(train_gen_fast) \n",
    "\n",
    "print('pipeline complete, ready for training')\n",
    "print(f'shape of one batch of features: {sample_fast_batch_features.shape}') # 32 per batch and 10 length\n",
    "print(f'shape of one batch of labels: {sample_fast_batch_labels.shape}')  # 32 per batch\n",
    "print(f'example feature vector (first image in batch:\\n {sample_fast_batch_features[0]})') # 10 arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6fc714",
   "metadata": {},
   "source": [
    "### fast is unreasonably fast with only 1.6s, since a detector + descriptor combo usually will take a while\n",
    "so an isolated evaluation will be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6665b90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Analyzing Cracked Images ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cracked: 100%|██████████| 100/100 [00:05<00:00, 19.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analyzing Uncracked Images ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uncracked: 100%|██████████| 100/100 [00:05<00:00, 16.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- FINAL DIAGNOSTIC REPORT ---\n",
      "Average descriptors for CRACKED images: 239.87\n",
      "Average descriptors for UNCRACKED images: 55.82\n",
      "\n",
      "Your 'hero' image had 99 descriptors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_SIZE = 100\n",
    "\n",
    "fast = cv2.FastFeatureDetector_create(nonmaxSuppression=False)\n",
    "brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()\n",
    "\n",
    "def get_descriptor_count(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    keypoints_found = fast.detect(gray, None)\n",
    "    keypoints_kept, descriptors = brief.compute(gray, keypoints_found)\n",
    "    \n",
    "    return len(descriptors) if descriptors is not None else 0\n",
    "\n",
    "print(\"--- Analyzing Cracked Images ---\")\n",
    "positive_path = os.path.join(DATASET_PATH, 'Positive')\n",
    "positive_files = [os.path.join(positive_path, fname) for fname in os.listdir(positive_path)[:SAMPLE_SIZE]]\n",
    "positive_counts = [get_descriptor_count(path) for path in tqdm(positive_files, desc=\"Cracked\")]\n",
    "\n",
    "print(\"\\n--- Analyzing Uncracked Images ---\")\n",
    "negative_path = os.path.join(DATASET_PATH, 'Negative')\n",
    "negative_files = [os.path.join(negative_path, fname) for fname in os.listdir(negative_path)[:SAMPLE_SIZE]]\n",
    "negative_counts = [get_descriptor_count(path) for path in tqdm(negative_files, desc=\"Uncracked\")]\n",
    "\n",
    "print(\"\\n\\n--- FINAL DIAGNOSTIC REPORT ---\")\n",
    "print(f\"Average descriptors for CRACKED images: {np.mean(positive_counts):.2f}\")\n",
    "print(f\"Average descriptors for UNCRACKED images: {np.mean(negative_counts):.2f}\")\n",
    "print(f\"\\nYour 'hero' image had {positive_counts[0]} descriptors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62121be",
   "metadata": {},
   "source": [
    "--- FINAL DIAGNOSTIC REPORT ---\n",
    "\n",
    "Average descriptors for CRACKED images: 239.87\n",
    "\n",
    "Average descriptors for UNCRACKED images: 55.82\n",
    "\n",
    "Your 'hero' image had 99 descriptors.\n",
    "\n",
    "the result is pleasantly surprising with a discriminative score of ~4.3 (239 / 55). A nice baseline to have, but the detector was still picking up a significant amount of background texture noise, so to improve this, a higher threshold (30) will be trialed if it's true or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e7a3c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Analyzing Cracked Images ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cracked: 100%|██████████| 100/100 [00:03<00:00, 25.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analyzing Uncracked Images ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uncracked: 100%|██████████| 100/100 [00:03<00:00, 25.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- FINAL DIAGNOSTIC REPORT ---\n",
      "Average descriptors for CRACKED images: 49.96\n",
      "Average descriptors for UNCRACKED images: 2.40\n",
      "\n",
      "Your 'hero' image had 23 descriptors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_SIZE = 100\n",
    "\n",
    "fast = cv2.FastFeatureDetector_create(threshold=30, nonmaxSuppression=False)\n",
    "brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()\n",
    "\n",
    "def get_descriptor_count(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    keypoints_found = fast.detect(gray, None)\n",
    "    keypoints_kept, descriptors = brief.compute(gray, keypoints_found)\n",
    "    \n",
    "    return len(descriptors) if descriptors is not None else 0\n",
    "\n",
    "print(\"--- Analyzing Cracked Images ---\")\n",
    "positive_path = os.path.join(DATASET_PATH, 'Positive')\n",
    "positive_files = [os.path.join(positive_path, fname) for fname in os.listdir(positive_path)[:SAMPLE_SIZE]]\n",
    "positive_counts = [get_descriptor_count(path) for path in tqdm(positive_files, desc=\"Cracked\")]\n",
    "\n",
    "print(\"\\n--- Analyzing Uncracked Images ---\")\n",
    "negative_path = os.path.join(DATASET_PATH, 'Negative')\n",
    "negative_files = [os.path.join(negative_path, fname) for fname in os.listdir(negative_path)[:SAMPLE_SIZE]]\n",
    "negative_counts = [get_descriptor_count(path) for path in tqdm(negative_files, desc=\"Uncracked\")]\n",
    "\n",
    "print(\"\\n\\n--- FINAL DIAGNOSTIC REPORT ---\")\n",
    "print(f\"Average descriptors for CRACKED images: {np.mean(positive_counts):.2f}\")\n",
    "print(f\"Average descriptors for UNCRACKED images: {np.mean(negative_counts):.2f}\")\n",
    "print(f\"\\nYour 'hero' image had {positive_counts[0]} descriptors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf22e07",
   "metadata": {},
   "source": [
    "--- FINAL DIAGNOSTIC REPORT ---\n",
    "\n",
    "Average descriptors for CRACKED images: 49.96\n",
    "\n",
    "Average descriptors for UNCRACKED images: 2.40\n",
    "\n",
    "Your 'hero' image had 23 descriptors.\n",
    "\n",
    "As expected, a higher threshold will yield a better result. So for the comparison, FAST will use a threshold of 30 because it can distinguish cracks from paint texture. This calibration will be applied to both FAST and ORB (which also utilizes FAST in its algorithm) to ensure they were operating effectively on the dataset. Adaptive algorithms like AKAZE and MSER were evaluated at default settings as they inherently handle noise variation through their internal scale-space mechanisms inherently handle the local contrast variations without manual tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4762957",
   "metadata": {},
   "source": [
    "## orb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b7370b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generator_orb(image_paths, labels, batch_size):\n",
    "    orb = cv2.ORB_create(fastThreshold=30)\n",
    "    \n",
    "    num_samples = len(image_paths)\n",
    "    \n",
    "    while True:\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        shuffled_paths = image_paths[indices]\n",
    "        shuffled_labels = labels[indices]\n",
    "        \n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_paths = shuffled_paths[i:i + batch_size]\n",
    "            batch_labels = shuffled_labels[i:i + batch_size]\n",
    "            \n",
    "            batch_features = []\n",
    "            \n",
    "            for img_path in tqdm(batch_paths, desc='Batch Progress'):\n",
    "                image = cv2.imread(img_path)\n",
    "                image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "                gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                keypoints, descriptors = orb.detectAndCompute(gray_image, None)\n",
    "                \n",
    "                if descriptors is not None:\n",
    "                    feature_vector = np.mean(descriptors, axis=0)\n",
    "                else:\n",
    "                    feature_vector = np.zeros(32)\n",
    "                \n",
    "                batch_features.append(feature_vector)\n",
    "            \n",
    "            yield np.array(batch_features), np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "13da0332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching one batch of feature vectors to test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline complete, ready for training\n",
      "shape of one batch of features: (32, 32)\n",
      "shape of one batch of labels: (32,)\n",
      "example feature vector (first image in batch:\n",
      " [ 47.10714286 136.82142857 100.78571429 174.78571429  89.42857143\n",
      " 115.57142857 141.42857143 147.03571429 103.53571429 103.71428571\n",
      " 116.64285714 144.57142857 107.35714286 116.         146.60714286\n",
      " 109.5        136.82142857 129.89285714  98.67857143 138.14285714\n",
      " 160.92857143 142.46428571  58.46428571 191.78571429 112.96428571\n",
      " 127.03571429 158.42857143 144.67857143  79.64285714 130.67857143\n",
      " 135.32142857 116.85714286])\n",
      "Label of the zero-vector image: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Verification\n",
    "train_gen_orb = feature_generator_orb(X_train_paths, y_train, BATCH_SIZE)\n",
    "\n",
    "print('fetching one batch of feature vectors to test')\n",
    "sample_orb_batch_features, sample_orb_batch_labels = next(train_gen_orb) \n",
    "\n",
    "print('pipeline complete, ready for training')\n",
    "print(f'shape of one batch of features: {sample_orb_batch_features.shape}') # 32 per batch and 10 length\n",
    "print(f'shape of one batch of labels: {sample_orb_batch_labels.shape}')  # 32 per batch\n",
    "print(f'example feature vector (first image in batch:\\n {sample_orb_batch_features[0]})') # 10 arrays\n",
    "print(f\"Label of the zero-vector image: {sample_orb_batch_labels[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "125e972e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing ORB Configuration: Default ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cracked: 100%|██████████| 1000/1000 [00:42<00:00, 23.79it/s]\n",
      "Uncracked: 100%|██████████| 1000/1000 [00:45<00:00, 21.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Cracked Avg: 139.38\n",
      "  -> Uncracked Avg: 39.13\n",
      "  -> Ratio: 3.56\n",
      "\n",
      "--- Testing ORB Configuration: Calibrated (Threshold 30) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cracked: 100%|██████████| 1000/1000 [00:41<00:00, 23.92it/s]\n",
      "Uncracked: 100%|██████████| 1000/1000 [00:45<00:00, 21.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Cracked Avg: 87.53\n",
      "  -> Uncracked Avg: 18.99\n",
      "  -> Ratio: 4.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_SIZE = 1000  \n",
    "\n",
    "def test_orb_params(fast_thresh=None, name=\"Default\"):\n",
    "    print(f\"\\n--- Testing ORB Configuration: {name} ---\")\n",
    "    \n",
    "    if fast_thresh:\n",
    "        orb = cv2.ORB_create(fastThreshold=fast_thresh)\n",
    "    else:\n",
    "        orb = cv2.ORB_create() \n",
    "\n",
    "    positive_path = os.path.join(DATASET_PATH, 'Positive')\n",
    "    negative_path = os.path.join(DATASET_PATH, 'Negative')\n",
    "    \n",
    "    pos_files = [os.path.join(positive_path, f) for f in os.listdir(positive_path)[:SAMPLE_SIZE]]\n",
    "    neg_files = [os.path.join(negative_path, f) for f in os.listdir(negative_path)[:SAMPLE_SIZE]]\n",
    "\n",
    "    pos_counts = []\n",
    "    for p in tqdm(pos_files, desc=\"Cracked\"):\n",
    "        img = cv2.imread(p); img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        kps, desc = orb.detectAndCompute(gray, None)\n",
    "        count = len(desc) if desc is not None else 0\n",
    "        pos_counts.append(count)\n",
    "\n",
    "    neg_counts = []\n",
    "    for p in tqdm(neg_files, desc=\"Uncracked\"):\n",
    "        img = cv2.imread(p); img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        kps, desc = orb.detectAndCompute(gray, None)\n",
    "        count = len(desc) if desc is not None else 0\n",
    "        neg_counts.append(count)\n",
    "\n",
    "    avg_pos = np.mean(pos_counts)\n",
    "    avg_neg = np.mean(neg_counts)\n",
    "    ratio = avg_pos / (avg_neg + 1e-6)\n",
    "    \n",
    "    print(f\"  -> Cracked Avg: {avg_pos:.2f}\")\n",
    "    print(f\"  -> Uncracked Avg: {avg_neg:.2f}\")\n",
    "    print(f\"  -> Ratio: {ratio:.2f}\")\n",
    "\n",
    "test_orb_params(None, 'Default')\n",
    "test_orb_params(fast_thresh=30, name=\"Calibrated (Threshold 30)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125183f1",
   "metadata": {},
   "source": [
    "```\n",
    "--- Testing ORB Configuration: Default ---\n",
    "-> Cracked Avg: 139.38\n",
    "  -> Uncracked Avg: 39.13\n",
    "  -> Ratio: 3.56\n",
    "\n",
    "--- Testing ORB Configuration: Calibrated (Threshold 30) ---\n",
    "-> Cracked Avg: 87.53\n",
    "  -> Uncracked Avg: 18.99\n",
    "  -> Ratio: 4.61\n",
    "  \n",
    "the improvement isn't as great as FAST most probably because ORB can handle a noise somewhat in it's inherent algorithm, but this is an improvement nonetheless so the new threshold will be used\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8968f406",
   "metadata": {},
   "source": [
    "## akaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3127e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generator_akaze(image_paths, labels, batch_size):\n",
    "    akaze = cv2.AKAZE_create()\n",
    "    \n",
    "    num_samples = len(image_paths)\n",
    "    \n",
    "    while True:\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        shuffled_paths = image_paths[indices]\n",
    "        shuffled_labels = labels[indices]\n",
    "        \n",
    "        for i in range (0, num_samples, batch_size):\n",
    "            batch_paths = shuffled_paths[i:i + batch_size]\n",
    "            batch_labels = shuffled_labels[i:i + batch_size]\n",
    "            \n",
    "            batch_features = []\n",
    "            \n",
    "            for img_path in tqdm(batch_paths, desc='Batch Progress'):\n",
    "                image = cv2.imread(img_path)\n",
    "                image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "                gray_img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                keypoints, descriptors = akaze.detectAndCompute(gray_img, None)\n",
    "                \n",
    "                if descriptors is not None:\n",
    "                    feature_vectors = np.mean(descriptors, axis=0)\n",
    "                else:\n",
    "                    feature_vectors = np.zeros(61)\n",
    "                \n",
    "                batch_features.append(feature_vectors)\n",
    "            \n",
    "            yield np.array(batch_features), np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2dfb44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching one batch of feature vectors to test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 19.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline complete, ready for training\n",
      "shape of one batch of features: (32, 61)\n",
      "shape of one batch of labels: (32,)\n",
      "example feature vector (first image in batch:\n",
      " [124.78947368  26.63157895 120.47368421  86.89473684 147.73684211\n",
      "  14.42105263 132.         143.68421053 129.52631579  75.05263158\n",
      " 157.57894737  52.21052632  43.73684211  84.         230.89473684\n",
      " 104.57894737 181.68421053 169.26315789 137.52631579 226.05263158\n",
      " 105.94736842 199.78947368 146.68421053  73.94736842  13.10526316\n",
      "  35.89473684  39.94736842  37.68421053  63.68421053  13.89473684\n",
      " 126.68421053  94.42105263 154.94736842 119.         107.63157895\n",
      " 125.26315789 113.05263158 126.89473684  82.10526316  85.57894737\n",
      " 137.63157895 156.47368421  84.57894737 117.68421053 160.\n",
      " 109.05263158  34.68421053 148.42105263  38.52631579 198.78947368\n",
      "  70.68421053  24.73684211 101.94736842 100.73684211  28.84210526\n",
      "  20.73684211  35.78947368 221.63157895 212.31578947 242.36842105\n",
      "  35.73684211])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Verification\n",
    "train_gen_akaze = feature_generator_akaze(X_train_paths, y_train, BATCH_SIZE)\n",
    "\n",
    "print('fetching one batch of feature vectors to test')\n",
    "sample_akaze_batch_features, sample_akaze_batch_labels = next(train_gen_akaze) \n",
    "\n",
    "print('pipeline complete, ready for training')\n",
    "print(f'shape of one batch of features: {sample_akaze_batch_features.shape}') # 32 per batch and 10 length\n",
    "print(f'shape of one batch of labels: {sample_akaze_batch_labels.shape}')  # 32 per batch\n",
    "print(f'example feature vector (first image in batch:\\n {sample_akaze_batch_features[0]})') # 10 arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03030c2",
   "metadata": {},
   "source": [
    "## HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3309b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generator_hog(image_paths, labels, batch_size):\n",
    "    num_samples = len(image_paths)\n",
    "    \n",
    "    while True:\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        shuffled_paths = image_paths[indices]\n",
    "        shuffled_labels = labels[indices]\n",
    "        \n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_paths = shuffled_paths[i: i + batch_size]\n",
    "            batch_labels = shuffled_labels[i: i + batch_size]\n",
    "            \n",
    "            batch_features = []\n",
    "            \n",
    "            for img_path in batch_paths:\n",
    "                image = cv2.imread(img_path)\n",
    "                image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "                gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                hog_features = hog(\n",
    "                    gray_image,\n",
    "                    orientations=9,\n",
    "                    pixels_per_cell=(8,8),\n",
    "                    cells_per_block=(2,2),\n",
    "                    transform_sqrt=True,\n",
    "                    block_norm='L1'\n",
    "                )\n",
    "                \n",
    "                batch_features.append(hog_features)\n",
    "            \n",
    "            yield np.array(batch_features), np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ab23f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching one batch of feature vectors to test\n",
      "pipeline complete, ready for training\n",
      "shape of one batch of features: (32, 26244)\n",
      "shape of one batch of labels: (32,)\n",
      "example feature vector (first image in batch:\n",
      " [0.03495404 0.02094397 0.02161804 ... 0.02134337 0.006198   0.02092129])\n"
     ]
    }
   ],
   "source": [
    "# Verification\n",
    "train_gen_hog = feature_generator_hog(X_train_paths, y_train, BATCH_SIZE)\n",
    "\n",
    "print('fetching one batch of feature vectors to test')\n",
    "sample_hog_batch_features, sample_hog_batch_labels = next(train_gen_hog) \n",
    "\n",
    "print('pipeline complete, ready for training')\n",
    "print(f'shape of one batch of features: {sample_hog_batch_features.shape}') # 32 per batch and 10 length\n",
    "print(f'shape of one batch of labels: {sample_hog_batch_labels.shape}')  # 32 per batch\n",
    "print(f'example feature vector (first image in batch:\\n {sample_hog_batch_features[0]})') # 10 arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39941669",
   "metadata": {},
   "source": [
    "## MSER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae85a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generator_mser(image_paths, labels, batch_size):\n",
    "    mser = cv2.MSER_create()\n",
    "    \n",
    "    num_samples = len(image_paths)\n",
    "    \n",
    "    while True:\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        shuffled_paths = image_paths[indices]\n",
    "        shuffled_labels = labels[indices]\n",
    "        \n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_paths = shuffled_paths[i:i + batch_size]\n",
    "            batch_labels = shuffled_labels[i:i + batch_size]\n",
    "            \n",
    "            batch_features = []\n",
    "            \n",
    "            for img_path in batch_paths:\n",
    "                image = cv2.imread(img_path)\n",
    "                image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "                gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                regions, _ = mser.detectRegions(gray_image)\n",
    "                \n",
    "                total_area = 0\n",
    "                \n",
    "                if regions is not None:\n",
    "                    total_area = sum(cv2.contourArea(region) for region in regions)\n",
    "                \n",
    "                batch_features.append([total_area])\n",
    "            \n",
    "            yield np.array(batch_features), np.array(batch_labels)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c15650b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching one batch of feature vectors to test\n",
      "pipeline complete, ready for training\n",
      "shape of one batch of features: (32, 1)\n",
      "shape of one batch of labels: (32,)\n",
      "example feature vector (first image in batch:\n",
      " [106654.5])\n"
     ]
    }
   ],
   "source": [
    "# Verification\n",
    "train_gen_mser = feature_generator_mser(X_train_paths, y_train, BATCH_SIZE)\n",
    "\n",
    "print('fetching one batch of feature vectors to test')\n",
    "sample_mser_batch_features, sample_mser_batch_labels = next(train_gen_mser) \n",
    "\n",
    "print('pipeline complete, ready for training')\n",
    "print(f'shape of one batch of features: {sample_mser_batch_features.shape}') # 32 per batch and 10 length\n",
    "print(f'shape of one batch of labels: {sample_mser_batch_labels.shape}')  # 32 per batch\n",
    "print(f'example feature vector (first image in batch:\\n {sample_mser_batch_features[0]})') # 10 arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78e780c",
   "metadata": {},
   "source": [
    "# pre test\n",
    "a test for all the local descriptor pipelines first so that the pipelines can be evaluated first before doing a rigorous training using model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d3ebad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing FAST + BRIEF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cracked (FAST + BRIEF): 100%|██████████| 1325/1325 [01:18<00:00, 16.90it/s] \n",
      "Uncracked (FAST + BRIEF): 100%|██████████| 1325/1325 [01:46<00:00, 12.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing ORB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cracked (ORB): 100%|██████████| 1325/1325 [01:07<00:00, 19.52it/s] \n",
      "Uncracked (ORB): 100%|██████████| 1325/1325 [01:19<00:00, 16.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing AKAZE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cracked (AKAZE): 100%|██████████| 1325/1325 [01:07<00:00, 19.65it/s] \n",
      "Uncracked (AKAZE): 100%|██████████| 1325/1325 [01:20<00:00, 16.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing MSER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cracked (MSER): 100%|██████████| 1325/1325 [01:07<00:00, 19.71it/s] \n",
      "Uncracked (MSER): 100%|██████████| 1325/1325 [01:22<00:00, 16.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final pre-test diagnostic report\n",
      "algorithm: FAST + BRIEF\n",
      "Avg features on CRACKED images: 4465.603773584906\n",
      "Avg features on UNCRACKED images: 8068.931320754717\n",
      "Discriminative ratio (Cracked/Uncracked): 0.5534318728857132\n",
      "algorithm: ORB\n",
      "Avg features on CRACKED images: 78.40754716981132\n",
      "Avg features on UNCRACKED images: 31.244528301886792\n",
      "Discriminative ratio (Cracked/Uncracked): 2.509480825018428\n",
      "algorithm: AKAZE\n",
      "Avg features on CRACKED images: 4.794716981132075\n",
      "Avg features on UNCRACKED images: 1.9486792452830188\n",
      "Discriminative ratio (Cracked/Uncracked): 2.4604944770893953\n",
      "algorithm: MSER\n",
      "Avg features on CRACKED images: 32002.570188679245\n",
      "Avg features on UNCRACKED images: 23418.564150943395\n",
      "Discriminative ratio (Cracked/Uncracked): 1.3665470684300474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_SIZE = 10000\n",
    "\n",
    "def get_descriptor_count_fast(image_path):\n",
    "    fast = cv2.FastFeatureDetector_create(threshold=30, nonmaxSuppression=True)\n",
    "    brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()\n",
    "    img = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    keypoints = fast.detect(gray, None)\n",
    "    _, descriptors = brief.compute(gray, keypoints)\n",
    "    return len(descriptors) if descriptors is not None else 0\n",
    "\n",
    "def get_descriptor_count_orb(image_path):\n",
    "    orb = cv2.ORB_create(fastThreshold=30)\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, descriptors = orb.detectAndCompute(gray, None)\n",
    "    return len(descriptors) if descriptors is not None else 0\n",
    "\n",
    "def get_descriptor_count_akaze(image_path):\n",
    "    akaze = cv2.AKAZE_create()\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, descriptors = akaze.detectAndCompute(gray, None)\n",
    "    return len(descriptors) if descriptors is not None else 0\n",
    "\n",
    "def get_area_mser(image_path):\n",
    "    mser = cv2.MSER_create()\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    regions, _ = mser.detectRegions(gray)\n",
    "    return sum(cv2.contourArea(r) for r in regions) if regions is not None else 0\n",
    "\n",
    "results = {}\n",
    "diagnostics_to_run = {\n",
    "    'FAST + BRIEF': get_descriptor_count_fast,\n",
    "    'ORB': get_descriptor_count_orb,\n",
    "    'AKAZE': get_descriptor_count_akaze,\n",
    "    'MSER': get_area_mser\n",
    "}\n",
    "\n",
    "positive_files = [os.path.join(positive_path, fname) for fname in os.listdir(positive_path)[:SAMPLE_SIZE]]\n",
    "negative_files = [os.path.join(negative_path, fname) for fname in os.listdir(negative_path)[:SAMPLE_SIZE]]\n",
    "\n",
    "for name, helper_function in diagnostics_to_run.items():\n",
    "    print(f'analyzing {name}')\n",
    "    \n",
    "    positive_counts = [helper_function(path) for path in tqdm(positive_files, desc=f'Cracked ({name})')]\n",
    "    negative_counts = [helper_function(path) for path in tqdm(negative_files, desc=f'Uncracked ({name})')]\n",
    "    \n",
    "    results[name] = {\n",
    "        'cracked_avg': np.mean(positive_counts),\n",
    "        'uncracked_avg': np.mean(negative_counts)\n",
    "    }\n",
    "\n",
    "print('final pre-test diagnostic report')\n",
    "\n",
    "for name, data in results.items():\n",
    "    cracked_avg = data['cracked_avg']\n",
    "    uncracked_avg = data['uncracked_avg']\n",
    "    \n",
    "    ratio = cracked_avg / (uncracked_avg + 1e-6)\n",
    "    \n",
    "    print(f'algorithm: {name}')\n",
    "    print(f'Avg features on CRACKED images: {cracked_avg}')\n",
    "    print(f'Avg features on UNCRACKED images: {uncracked_avg}')\n",
    "    print(f'Discriminative ratio (Cracked/Uncracked): {ratio}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec50e11",
   "metadata": {},
   "source": [
    "```\n",
    "final pre-test diagnostic report\n",
    "algorithm: FAST + BRIEF\n",
    "Avg features on CRACKED images: 4465.603773584906\n",
    "Avg features on UNCRACKED images: 8068.931320754717\n",
    "Discriminative ratio (Cracked/Uncracked): 0.5534318728857132\n",
    "algorithm: ORB\n",
    "Avg features on CRACKED images: 78.40754716981132\n",
    "Avg features on UNCRACKED images: 31.244528301886792\n",
    "Discriminative ratio (Cracked/Uncracked): 2.509480825018428\n",
    "algorithm: AKAZE\n",
    "Avg features on CRACKED images: 4.794716981132075\n",
    "Avg features on UNCRACKED images: 1.9486792452830188\n",
    "Discriminative ratio (Cracked/Uncracked): 2.4604944770893953\n",
    "algorithm: MSER\n",
    "Avg features on CRACKED images: 32002.570188679245\n",
    "Avg features on UNCRACKED images: 23418.564150943395\n",
    "Discriminative ratio (Cracked/Uncracked): 1.3665470684300474\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5db620",
   "metadata": {},
   "source": [
    "## result:\n",
    "FAST and MSER will be excluded from the final evaluation. The final score test will be a competition between our top candidates: AKAZE, ORB, LBP, and HOG\n",
    "\n",
    "### Why FAST and MSER are excluded?\n",
    "Both of these algorithms has a very bad ratio ~1 or even less, meaning that for every one flag of signal, there is also 1 or more noise. Meaning it can't distinguish a crack from a shadow\n",
    "  \n",
    "### Hypothesis: a Global Descriptor will be better than a Local one\n",
    "The relatively low results of all the keypoint detectors (< 3.0) suggests the 'local descriptor hypothesis': that cracks are best defined by specific high-contrast points, is weak for this specific dataset due to the high texture noise in here\n",
    "\n",
    "Consequently, the scope of the final evaluation has been expanded to prioritize 'Global Descriptors' like LBP (Global Texture) and HOG (Global Shape). The updated hypothesis is that analyzing the overall texture or edge distribution of the entire image will be more robust against local paint irregularities than trying to isolate the individual keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f44a5b5",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5617b8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline model for: LBP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 17.32it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:02<00:00, 13.62it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 16.41it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:02<00:00, 15.02it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 16.97it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 17.53it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 18.43it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 19.31it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 16.96it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 17.01it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 16.50it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 21.06it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 21.50it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 22.27it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.28it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 21.62it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.36it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 20.88it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.02it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 16.51it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 17.61it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 16.50it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 19.35it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:02<00:00, 14.65it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 19.44it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 17.37it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 17.62it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 17.96it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 18.06it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 17.31it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 19.45it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 20.30it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 18.40it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 19.11it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 17.44it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:02<00:00, 15.98it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 16.74it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 17.16it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 16.57it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 16.50it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 18.49it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 17.77it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 18.87it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 18.31it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 19.86it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:02<00:00, 14.99it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:02<00:00, 13.77it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:02<00:00, 15.12it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 16.33it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 17.86it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 17.48it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:02<00:00, 14.61it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:02<00:00, 15.38it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 16.44it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 17.59it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 18.12it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 16.73it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 18.68it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 18.86it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 18.75it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 17.32it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 17.93it/s]\n",
      "Training LBP: 100%|██████████| 62/62 [01:52<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting test features for LBP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing LBP: 100%|██████████| 663/663 [00:37<00:00, 17.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline model for: HOG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training HOG: 100%|██████████| 62/62 [01:58<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting test features for HOG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing HOG: 100%|██████████| 663/663 [00:33<00:00, 19.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline model for: AKAZE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 22.41it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 22.03it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 21.16it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.22it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.28it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.08it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 20.66it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 21.07it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.64it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 19.55it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 20.93it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 20.68it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 20.82it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 19.72it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.82it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 22.14it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.95it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 19.98it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 20.84it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 20.62it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 21.16it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 20.03it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.60it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.40it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.70it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.11it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 21.17it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.05it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 22.00it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 21.34it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 20.98it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 20.77it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 21.44it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 21.65it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 21.51it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 21.71it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 22.02it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 21.94it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 21.61it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 21.59it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 21.13it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 21.91it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.12it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 22.15it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 26.04it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 22.22it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.15it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 22.13it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 21.43it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 22.04it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 22.71it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.02it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.93it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 25.10it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.65it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 22.34it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 22.56it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 22.59it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.20it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.53it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.43it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 21.53it/s]\n",
      "Training AKAZE: 100%|██████████| 62/62 [01:29<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting test features for AKAZE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing AKAZE: 100%|██████████| 663/663 [00:29<00:00, 22.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training baseline model for: ORB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.12it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.87it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 22.68it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.15it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.59it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 25.20it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.39it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.50it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.50it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 22.32it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.88it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.71it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.88it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 22.46it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.83it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.94it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.44it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 22.08it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 22.06it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 25.07it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.97it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.64it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 22.75it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.67it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.41it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 22.82it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 22.81it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.73it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 22.88it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 26.17it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.90it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.08it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 25.79it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 25.93it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.29it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.42it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 22.78it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 22.93it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.80it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 22.72it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.51it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.79it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 21.95it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.58it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.70it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 25.65it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 26.33it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.48it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.90it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.44it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 26.95it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.92it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 25.09it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 25.66it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.83it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.87it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.44it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 25.02it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.57it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 23.63it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 22.69it/s]\n",
      "Batch Progress: 100%|██████████| 32/32 [00:01<00:00, 24.89it/s]\n",
      "Training ORB: 100%|██████████| 62/62 [01:22<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting test features for ORB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing ORB: 100%|██████████| 663/663 [00:27<00:00, 23.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final baseline comparison report\n",
      "==========================================\n",
      "               LBP \n",
      "==========================================\n",
      "              precision    recall  f1-score   support\n",
      "No Crack (0)      0.53         0.94      0.68      332.0\n",
      "   Crack (1)      0.74         0.16      0.26      331.0\n",
      "\n",
      "   Accuracy                           0.55     663.0\n",
      "   Macro Avg      0.63         0.55      0.47      663.0\n",
      "Weighted Avg      0.63         0.55      0.47      663.0\n",
      "==========================================\n",
      "               HOG WINNER\n",
      "==========================================\n",
      "              precision    recall  f1-score   support\n",
      "No Crack (0)      0.73         0.58      0.65      332.0\n",
      "   Crack (1)      0.65         0.78      0.71      331.0\n",
      "\n",
      "   Accuracy                           0.68     663.0\n",
      "   Macro Avg      0.69         0.68      0.68      663.0\n",
      "Weighted Avg      0.69         0.68      0.68      663.0\n",
      "==========================================\n",
      "               AKAZE \n",
      "==========================================\n",
      "              precision    recall  f1-score   support\n",
      "No Crack (0)      0.55         0.96      0.70      332.0\n",
      "   Crack (1)      0.84         0.21      0.34      331.0\n",
      "\n",
      "   Accuracy                           0.59     663.0\n",
      "   Macro Avg      0.70         0.59      0.52      663.0\n",
      "Weighted Avg      0.70         0.59      0.52      663.0\n",
      "==========================================\n",
      "               ORB \n",
      "==========================================\n",
      "              precision    recall  f1-score   support\n",
      "No Crack (0)      0.50         0.98      0.66      332.0\n",
      "   Crack (1)      0.30         0.01      0.02      331.0\n",
      "\n",
      "   Accuracy                           0.49     663.0\n",
      "   Macro Avg      0.40         0.49      0.34      663.0\n",
      "Weighted Avg      0.40         0.49      0.34      663.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAINING_STEPS = len(X_train_paths) // BATCH_SIZE\n",
    "\n",
    "pipeline_generators = {\n",
    "    'LBP': feature_generator_lbp,\n",
    "    'HOG': feature_generator_hog,\n",
    "    'AKAZE': feature_generator_akaze,\n",
    "    'ORB': feature_generator_orb \n",
    "}\n",
    "\n",
    "def extract_test_features(extractor_name, paths):\n",
    "    print(f'extracting test features for {extractor_name}')\n",
    "    \n",
    "    test_features = []\n",
    "    \n",
    "    akaze = cv2.AKAZE_create()\n",
    "    orb = cv2.ORB_create(fastThreshold=30)\n",
    "    \n",
    "    for img_path in tqdm(paths, desc=f'Testing {extractor_name}'):\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        if extractor_name == 'LBP':\n",
    "            lbp = local_binary_pattern(gray_image, P=8, R=1, method='uniform')\n",
    "            hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 11), range=(0, 10))\n",
    "            hist = hist.astype('float')\n",
    "            hist /= (hist.sum() + 1e-6)\n",
    "            test_features.append(hist)\n",
    "        elif extractor_name == 'HOG':\n",
    "            hog_features = hog(gray_image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), transform_sqrt=True, block_norm='L1')\n",
    "            test_features.append(hog_features)\n",
    "        elif extractor_name == 'AKAZE':\n",
    "            _, descriptors = akaze.detectAndCompute(gray_image, None)\n",
    "            if descriptors is not None: \n",
    "                feature_vector = np.mean(descriptors, axis=0)\n",
    "            else:\n",
    "                feature_vector = np.zeros(61)\n",
    "            test_features.append(feature_vector)\n",
    "        elif extractor_name == 'ORB':\n",
    "            _, descriptors = orb.detectAndCompute(gray_image, None)\n",
    "            feature_vector = np.mean(descriptors, axis=0) if descriptors is not None else np.zeros(32)\n",
    "            test_features.append(feature_vector)\n",
    "        \n",
    "    return np.array(test_features)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for pipeline_name, generator_function in pipeline_generators.items():\n",
    "    print(f'training baseline model for: {pipeline_name}')\n",
    "    \n",
    "    model = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)\n",
    "    train_generator = generator_function(X_train_paths, y_train, BATCH_SIZE)\n",
    "    \n",
    "    for _ in tqdm(range(int(NUM_TRAINING_STEPS)), desc=f'Training {pipeline_name}'):\n",
    "        X_batch, y_batch = next(train_generator)\n",
    "        model.partial_fit(X_batch, y_batch, classes=np.array([0, 1]))\n",
    "    \n",
    "    X_test_features = extract_test_features(pipeline_name, X_test_paths)\n",
    "    y_pred = model.predict(X_test_features)\n",
    "    report = classification_report(y_test, y_pred, target_names=['No Crack (0)', 'Crack (1)'], output_dict=True)\n",
    "    results[pipeline_name] = report\n",
    "\n",
    "print('final baseline comparison report')\n",
    "\n",
    "best_pipeline = max(results, key=lambda p: results[p]['macro avg']['f1-score'])\n",
    "\n",
    "for pipeline_name, report in results.items():\n",
    "    f1_crack = report['Crack (1)']['f1-score']\n",
    "    print(\"==========================================\")\n",
    "    print(f\"               {pipeline_name} {'WINNER' if pipeline_name == best_pipeline else ''}\")\n",
    "    print(\"==========================================\")\n",
    "    print(f\"              precision    recall  f1-score   support\")\n",
    "    print(f\"No Crack (0)      {report['No Crack (0)']['precision']:.2f}         {report['No Crack (0)']['recall']:.2f}      {report['No Crack (0)']['f1-score']:.2f}      {report['No Crack (0)']['support']}\")\n",
    "    print(f\"   Crack (1)      {report['Crack (1)']['precision']:.2f}         {report['Crack (1)']['recall']:.2f}      {report['Crack (1)']['f1-score']:.2f}      {report['Crack (1)']['support']}\")\n",
    "    print(f\"\\n   Accuracy                           {report['accuracy']:.2f}     {report['macro avg']['support']}\")\n",
    "    print(f\"   Macro Avg      {report['macro avg']['precision']:.2f}         {report['macro avg']['recall']:.2f}      {report['macro avg']['f1-score']:.2f}      {report['macro avg']['support']}\")\n",
    "    print(f\"Weighted Avg      {report['weighted avg']['precision']:.2f}         {report['weighted avg']['recall']:.2f}      {report['weighted avg']['f1-score']:.2f}      {report['weighted avg']['support']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7a32f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f4abefa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to ./Result/baseline_results.json\n",
      "saved successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open('./Result/baseline_results.json', 'r') as f:\n",
    "        loaded_results = json.load(f)\n",
    "\n",
    "    lbp_f1_score = loaded_results['LBP']['Crack (1)']['f1-score']\n",
    "    print(f\"The LBP F1-score for cracks was: {lbp_f1_score}\")\n",
    "except:\n",
    "    results_filename = './Result/baseline_results.json'\n",
    "    print(f'saving to {results_filename}')\n",
    "\n",
    "    with open(results_filename, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    print('saved successfully')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
